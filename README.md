# PHS-BERT
A Domain-Specific Pretrained Language Model for Public Health Surveillance on Social Media
<br/>
<br/>
This repository provides the PHS-BERT model, a transformer-based pretrained language model for the early detection of tasks related to public health surveillance on social media. Please refer to our paper [link] for more details.

## Download
We provide one version of pretrained weights. Pretraining was based on the original BERT code, and training details are described in our paper (To be Published). The currently available version of pretrained weights is as follows:
1. [PHS-BERT](https://drive.google.com/file/d/1RIzqFHPwx_Ro152dkHiKU9omKaZzrhFF/view?usp=sharing) - based on BERT-base model

## Installation

## Quick links

## Datasets
### Suicide
- R-SSD
### Stress
- Dreaddit
- SAD
### Health Mention
- PHM
- PHM
- HMC2019
- RHMD
### Vaccine Sentiment
- VS1
- VS2
### COVID Related
- Covid Lies
- Covid Category
- COVIDSentiA
- COVIDSentiB
- COVIDSentiC
### Depression
- eRISK T3
- Depression_Reddit_1
- eRisk19 T1
- Depression_Twitter_1
- Depression_Twitter_2
### Other Health Related
- PubHealth
- Abortion
- Amazon Health
- SMM4H T1
- SMM4H T2
- HRT

## Fine-tuning PHS-BERT

## Contact Information
If you have any questions, please submit a Github issue of contact Usman Naseem (usman.naseem@sydney.edu.au).
